"""
Statistics Service - Historical Probability Analysis
=====================================================
Servicio de an√°lisis de probabilidades hist√≥ricas basado en datos crudos.
Recalcula scores al vuelo para permitir cambios en la l√≥gica de an√°lisis.

Responsabilidades:
- Cargar dataset JSONL en pandas DataFrame
- Normalizar scores usando l√≥gica actual de an√°lisis
- Calcular probabilidades de √©xito por patr√≥n y score
- Proporcionar estad√≠sticas en tiempo real para alertas

Author: TradingView Pattern Monitor Team
"""
import os
import json
import pandas as pd
import numpy as np
from typing import Dict, Optional, List, Tuple
from datetime import datetime, timedelta
from pathlib import Path

from src.utils.logger import get_logger
from src.logic.analysis_service import analyze_trend


logger = get_logger(__name__)


class StatisticsService:
    """
    Servicio de an√°lisis estad√≠stico de se√±ales hist√≥ricas.
    
    Funcionalidades:
    - Carga de dataset JSONL con validaci√≥n
    - Normalizaci√≥n de scores usando l√≥gica actual
    - Queries de probabilidad con fuzzy matching
    - An√°lisis de rachas (streaks) de √©xito/fracaso
    """
    
    def __init__(self, data_path: str = "data/trading_signals_dataset.jsonl"):
        """
        Inicializa el servicio de estad√≠sticas.
        
        Args:
            data_path: Ruta al archivo JSONL con el dataset
        """
        self.data_path = Path(data_path)
        self.df: Optional[pd.DataFrame] = None
        self.last_load_time: Optional[datetime] = None
        self.records_loaded: int = 0
        
        # Cargar dataset al inicializar
        self._load_dataset()
        
        logger.info(
            f"üìä Statistics Service inicializado | "
            f"Registros: {self.records_loaded} | "
            f"Archivo: {self.data_path}"
        )
    
    def _load_dataset(self) -> None:
        """
        Carga el dataset JSONL en un DataFrame de pandas.
        Maneja casos donde el archivo no existe o est√° vac√≠o.
        """
        if not self.data_path.exists():
            logger.warning(f"‚ö†Ô∏è  Dataset no existe: {self.data_path}. Creando DataFrame vac√≠o.")
            self.df = pd.DataFrame()
            self.records_loaded = 0
            return
        
        try:
            # Leer JSONL l√≠nea por l√≠nea
            records = []
            with open(self.data_path, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line:  # Ignorar l√≠neas vac√≠as
                        try:
                            records.append(json.loads(line))
                        except json.JSONDecodeError as e:
                            logger.warning(f"‚ö†Ô∏è  L√≠nea JSONL inv√°lida ignorada: {e}")
            
            if not records:
                logger.warning(f"‚ö†Ô∏è  Dataset vac√≠o: {self.data_path}")
                self.df = pd.DataFrame()
                self.records_loaded = 0
                return
            
            # Crear DataFrame
            self.df = pd.DataFrame(records)
            self.records_loaded = len(self.df)
            self.last_load_time = datetime.utcnow()
            
            # Normalizar scores usando l√≥gica actual
            self._normalize_scores()
            
            logger.info(
                f"‚úÖ Dataset cargado | "
                f"Registros: {self.records_loaded} | "
                f"Columnas: {list(self.df.columns)}"
            )
            
        except Exception as e:
            logger.error(f"‚ùå Error al cargar dataset: {e}", exc_info=True)
            self.df = pd.DataFrame()
            self.records_loaded = 0
    
    def _normalize_scores(self) -> None:
        """
        Recalcula scores usando la l√≥gica actual de analyze_trend.
        Esto permite "viajar en el tiempo" y aplicar la l√≥gica de hoy
        a datos hist√≥ricos guardados con versiones anteriores del algoritmo.
        
        Crea/actualiza la columna 'calculated_score' en el DataFrame.
        """
        if self.df is None or self.df.empty:
            return
        
        # Verificar que existan las columnas de estructura V2
        if 'emas' not in self.df.columns or 'pattern_candle' not in self.df.columns:
            logger.warning("‚ö†Ô∏è  Estructura V2 no detectada. No se pueden recalcular scores.")
            self.df['calculated_score'] = np.nan
            return
        
        calculated_scores = []
        
        for idx, row in self.df.iterrows():
            try:
                # Extraer datos de la nueva estructura V2
                emas_data = row['emas']
                pattern_candle = row['pattern_candle']
                
                # Extraer close de pattern_candle
                close = pattern_candle.get('close')
                
                # Extraer EMAs
                emas = {
                    'ema_200': emas_data.get('ema_200'),
                    'ema_50': emas_data.get('ema_50'),
                    'ema_30': emas_data.get('ema_30'),
                    'ema_20': emas_data.get('ema_20')
                }
                
                # Validar que tengamos los datos necesarios
                if close is None or any(v is None for v in emas.values()):
                    calculated_scores.append(np.nan)
                    continue
                
                # Recalcular score usando l√≥gica actual
                trend_analysis = analyze_trend(close, emas)
                calculated_scores.append(trend_analysis.score)
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è  Error al recalcular score en fila {idx}: {e}")
                calculated_scores.append(np.nan)
        
        # Agregar columna al DataFrame
        self.df['calculated_score'] = calculated_scores
        
        valid_scores = sum(1 for s in calculated_scores if not pd.isna(s))
        logger.info(
            f"‚úÖ Scores recalculados | "
            f"V√°lidos: {valid_scores}/{len(calculated_scores)}"
        )
    
    def get_probability(
        self,
        pattern: str,
        current_score: int,
        current_alignment: Optional[str] = None,
        current_ema_order: Optional[str] = None,
        lookback_days: int = 30,
        score_tolerance: int = 1
    ) -> Dict[str, any]:
        """
        Calcula probabilidades con estad√≠sticas completas para alertas.
        Retorna estructura compatible con telegram_service y analysis_service.
        
        Args:
            pattern: Tipo de patr√≥n (ej: "SHOOTING_STAR", "HAMMER")
            current_score: Score de tendencia actual
            current_alignment: Alineaci√≥n actual (ej: "BULLISH_ALIGNED")
            current_ema_order: Orden expl√≠cito actual (ej: "P>20>30>50>200")
            lookback_days: Ventana de tiempo en d√≠as (default: 30)
            score_tolerance: Tolerancia para fuzzy matching (default: ¬±1)
            
        Returns:
            Dict con estructura:
            {
                "exact": {                # üéØ M√ÅXIMA PRECISI√ìN: Score exacto + EMA order exacto
                    "total_cases": int,
                    "verde_count": int,
                    "roja_count": int,
                    "verde_pct": float,
                    "roja_pct": float,
                    "expected_direction": str  # "VERDE" o "ROJA"
                },
                "by_alignment": {         # üìä PRECISI√ìN MEDIA: Score similar + mismo alignment
                    "total_cases": int,
                    "verde_count": int,
                    "roja_count": int,
                    "verde_pct": float,
                    "roja_pct": float,
                    "expected_direction": str,
                    "score_range": tuple
                },
                "by_score": {             # üìà M√ÅXIMA MUESTRA: Solo score similar
                    "total_cases": int,
                    "verde_count": int,
                    "roja_count": int,
                    "verde_pct": float,
                    "roja_pct": float,
                    "expected_direction": str,
                    "score_range": tuple
                },
                "streak": list,           # ["VERDE", "ROJA", "VERDE", ...]
                "lookback_days": int
            }
        """
        if self.df is None or self.df.empty:
            return self._empty_stats_response()
        
        # Filtrar por ventana de tiempo
        cutoff_date = datetime.utcnow() - timedelta(days=lookback_days)
        
        # Convertir timestamp a datetime si es string
        try:
            self.df['timestamp_dt'] = pd.to_datetime(self.df['timestamp'])
        except Exception:
            logger.warning("‚ö†Ô∏è  No se pudo parsear columna 'timestamp'")
            return self._empty_stats_response()
        
        df_filtered = self.df[self.df['timestamp_dt'] >= cutoff_date].copy()
        
        if df_filtered.empty:
            logger.info(f"üìä No hay datos en ventana de {lookback_days} d√≠as")
            return self._empty_stats_response()
        
        # Filtrar por patr√≥n exacto (nueva estructura V2)
        if 'pattern_candle' not in df_filtered.columns:
            logger.warning("‚ö†Ô∏è  Columna 'pattern_candle' no existe")
            return self._empty_stats_response()
        
        # Extraer patr√≥n del objeto pattern_candle
        df_filtered['pattern'] = df_filtered['pattern_candle'].apply(
            lambda x: x.get('pattern') if isinstance(x, dict) else None
        )
        
        df_filtered = df_filtered[df_filtered['pattern'] == pattern]
        
        if df_filtered.empty:
            logger.info(f"üìä No hay datos para patr√≥n {pattern}")
            return self._empty_stats_response()
        
        # Extraer columnas necesarias de la estructura V2
        df_filtered['success'] = df_filtered['outcome'].apply(
            lambda x: x.get('success') if isinstance(x, dict) else False
        )
        df_filtered['expected_direction'] = df_filtered['outcome'].apply(
            lambda x: x.get('expected_direction') if isinstance(x, dict) else None
        )
        df_filtered['actual_direction'] = df_filtered['outcome'].apply(
            lambda x: x.get('actual_direction') if isinstance(x, dict) else None
        )
        df_filtered['ema_order'] = df_filtered['emas'].apply(
            lambda x: x.get('ema_order') if isinstance(x, dict) else None
        )
        df_filtered['alignment'] = df_filtered['emas'].apply(
            lambda x: x.get('alignment') if isinstance(x, dict) else None
        )
        
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # 1. EXACTO: Score exacto + EMA order exacto
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        exact_stats = self._empty_single_stats()
        
        if current_ema_order:
            df_exact_order = df_filtered[
                (df_filtered['calculated_score'] == current_score) &
                (df_filtered['ema_order'] == current_ema_order)
            ]
            
            if not df_exact_order.empty:
                exact_stats = self._calculate_direction_stats(df_exact_order, pattern)
        
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # 2. BY_ALIGNMENT: Score similar + mismo alignment
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        score_min = current_score - score_tolerance
        score_max = current_score + score_tolerance
        
        by_alignment_stats = self._empty_single_stats()
        by_alignment_stats['score_range'] = (int(score_min), int(score_max))
        
        if current_alignment:
            df_by_alignment = df_filtered[
                (df_filtered['calculated_score'] >= score_min) &
                (df_filtered['calculated_score'] <= score_max) &
                (df_filtered['alignment'] == current_alignment)
            ]
            
            if not df_by_alignment.empty:
                by_alignment_stats = self._calculate_direction_stats(df_by_alignment, pattern)
                by_alignment_stats['score_range'] = (int(score_min), int(score_max))
        
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # 3. BY_SCORE: Solo score similar (m√°xima muestra)
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        df_by_score = df_filtered[
            (df_filtered['calculated_score'] >= score_min) &
            (df_filtered['calculated_score'] <= score_max)
        ]
        
        by_score_stats = self._empty_single_stats()
        by_score_stats['score_range'] = (int(score_min), int(score_max))
        
        if not df_by_score.empty:
            by_score_stats = self._calculate_direction_stats(df_by_score, pattern)
            by_score_stats['score_range'] = (int(score_min), int(score_max))
        
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # RACHA RECIENTE (basada en by_score para mayor muestra)
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        recent_directions = []
        if not df_by_score.empty:
            # Obtener √∫ltimas 5 direcciones actuales (m√°s recientes primero)
            recent_df = df_by_score.nlargest(5, 'timestamp_dt')
            recent_directions = recent_df['actual_direction'].tolist()
        
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # RESULTADO FINAL (3 niveles de precisi√≥n)
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        stats = {
            "exact": exact_stats,
            "by_alignment": by_alignment_stats,
            "by_score": by_score_stats,
            "streak": recent_directions,
            "lookback_days": lookback_days
        }
        
        logger.debug(
            f"üìä Estad√≠sticas | "
            f"Patr√≥n: {pattern} | "
            f"Score: {current_score} | "
            f"Exacto: {exact_stats['total_cases']} casos | "
            f"By Alignment: {by_alignment_stats['total_cases']} casos | "
            f"By Score: {by_score_stats['total_cases']} casos"
        )
        
        return stats
    
    def _calculate_direction_stats(self, df: pd.DataFrame, pattern: str) -> Dict[str, any]:
        """
        Calcula estad√≠sticas completas de direcciones para un DataFrame filtrado.
        
        Args:
            df: DataFrame con registros filtrados
            pattern: Tipo de patr√≥n para determinar direcci√≥n esperada
            
        Returns:
            Dict con estad√≠sticas completas
        """
        total_cases = len(df)
        
        # Contar direcciones (solo VERDE y ROJA)
        verde_count = (df['actual_direction'] == 'VERDE').sum()
        roja_count = (df['actual_direction'] == 'ROJA').sum()
        
        # Calcular porcentajes
        verde_pct = verde_count / total_cases if total_cases > 0 else 0.0
        roja_pct = roja_count / total_cases if total_cases > 0 else 0.0
        
        # Determinar direcci√≥n esperada del patr√≥n
        # SHOOTING_STAR y HANGING_MAN son bajistas ‚Üí esperan ROJA
        # HAMMER e INVERTED_HAMMER son alcistas ‚Üí esperan VERDE
        if pattern in ["SHOOTING_STAR", "HANGING_MAN"]:
            expected_direction = "ROJA"
        elif pattern in ["HAMMER", "INVERTED_HAMMER"]:
            expected_direction = "VERDE"
        else:
            expected_direction = "UNKNOWN"
        
        return {
            "total_cases": int(total_cases),
            "verde_count": int(verde_count),
            "roja_count": int(roja_count),
            "verde_pct": float(verde_pct),
            "roja_pct": float(roja_pct),
            "expected_direction": expected_direction
        }
    
    def _empty_single_stats(self) -> Dict[str, any]:
        """
        Retorna estructura de estad√≠sticas vac√≠a para un nivel.
        """
        return {
            "total_cases": 0,
            "verde_count": 0,
            "roja_count": 0,
            "verde_pct": 0.0,
            "roja_pct": 0.0,
            "expected_direction": "UNKNOWN"
        }
    
    def _empty_stats_response(self) -> Dict[str, any]:
        """
        Retorna respuesta vac√≠a cuando no hay datos disponibles.
        """
        exact_empty = self._empty_single_stats()
        by_alignment_empty = self._empty_single_stats()
        by_alignment_empty['score_range'] = (0, 0)
        by_score_empty = self._empty_single_stats()
        by_score_empty['score_range'] = (0, 0)
        
        return {
            "exact": exact_empty,
            "by_alignment": by_alignment_empty,
            "by_score": by_score_empty,
            "streak": [],
            "lookback_days": 0
        }
    
    def reload_dataset(self) -> None:
        """
        Recarga el dataset desde disco.
        √ötil para actualizar estad√≠sticas despu√©s de que se guarden nuevas se√±ales.
        """
        logger.info("üîÑ Recargando dataset...")
        self._load_dataset()
    
    def get_stats_summary(self) -> Dict[str, any]:
        """
        Retorna resumen general de estad√≠sticas del dataset.
        
        Returns:
            Dict con m√©tricas generales del dataset cargado
        """
        if self.df is None or self.df.empty:
            return {
                "records_loaded": 0,
                "last_load_time": None,
                "patterns_detected": {},
                "overall_win_rate": 0.0
            }
        
        # Contar patrones (nueva estructura V2)
        patterns = self.df['pattern_candle'].apply(
            lambda x: x.get('pattern') if isinstance(x, dict) else None
        ).value_counts().to_dict()
        
        # Win rate general
        success_rate = self.df['outcome'].apply(
            lambda x: x.get('success', False) if isinstance(x, dict) else False
        ).mean()
        
        return {
            "records_loaded": self.records_loaded,
            "last_load_time": self.last_load_time.isoformat() if self.last_load_time else None,
            "patterns_detected": patterns,
            "overall_win_rate": float(success_rate)
        }
