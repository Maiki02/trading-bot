"""
Ejemplo de cÃ³mo leer y analizar el dataset JSONL
=================================================
Este script demuestra cÃ³mo cargar y analizar los datos del dataset
generado por el sistema de backtesting.

JSONL (JSON Lines): Cada lÃ­nea es un JSON vÃ¡lido independiente.

Autor: TradingView Pattern Monitor Team
"""

import json
import pandas as pd
from pathlib import Path
from typing import List, Dict


def read_jsonl_simple(file_path: str) -> List[Dict]:
    """
    MÃ©todo 1: Lectura simple lÃ­nea por lÃ­nea.
    
    Args:
        file_path: Ruta al archivo JSONL
        
    Returns:
        Lista de diccionarios con los registros
    """
    records = []
    
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line:  # Ignorar lÃ­neas vacÃ­as
                record = json.loads(line)
                records.append(record)
    
    return records


def read_jsonl_pandas(file_path: str) -> pd.DataFrame:
    """
    MÃ©todo 2: Lectura con pandas (recomendado para anÃ¡lisis).
    
    Args:
        file_path: Ruta al archivo JSONL
        
    Returns:
        DataFrame de pandas
    """
    records = []
    
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line:
                try:
                    records.append(json.loads(line))
                except json.JSONDecodeError as e:
                    print(f"âš ï¸  LÃ­nea invÃ¡lida ignorada: {e}")
    
    return pd.DataFrame(records)


def analyze_dataset(df: pd.DataFrame):
    """
    Analiza el dataset y muestra estadÃ­sticas.
    
    Args:
        df: DataFrame con el dataset
    """
    print("=" * 80)
    print("ðŸ“Š ANÃLISIS DEL DATASET")
    print("=" * 80)
    
    # InformaciÃ³n bÃ¡sica
    print(f"\nðŸ“¦ Total de registros: {len(df)}")
    print(f"ðŸ“… Rango de fechas:")
    print(f"   Primer registro: {df['timestamp'].min()}")
    print(f"   Ãšltimo registro: {df['timestamp'].max()}")
    
    # Extraer columnas anidadas
    df['pattern'] = df['pattern_candle'].apply(lambda x: x['pattern'])
    df['confidence'] = df['pattern_candle'].apply(lambda x: x['confidence'])
    df['trend_score'] = df['emas'].apply(lambda x: x['trend_score'])
    df['alignment'] = df['emas'].apply(lambda x: x['alignment'])
    df['success'] = df['outcome'].apply(lambda x: x['success'])
    
    # EstadÃ­sticas por patrÃ³n
    print("\nðŸŽ¯ PATRONES DETECTADOS:")
    pattern_counts = df['pattern'].value_counts()
    for pattern, count in pattern_counts.items():
        pct = (count / len(df)) * 100
        print(f"   {pattern}: {count} ({pct:.1f}%)")
    
    # Win rate por patrÃ³n
    print("\nâœ… WIN RATE POR PATRÃ“N:")
    for pattern in df['pattern'].unique():
        pattern_df = df[df['pattern'] == pattern]
        win_rate = pattern_df['success'].mean() * 100
        wins = pattern_df['success'].sum()
        losses = len(pattern_df) - wins
        print(f"   {pattern}: {win_rate:.1f}% ({wins}W / {losses}L)")
    
    # EstadÃ­sticas de confianza
    print("\nðŸ’¯ ESTADÃSTICAS DE CONFIANZA:")
    print(f"   Media: {df['confidence'].mean():.3f}")
    print(f"   MÃ­nima: {df['confidence'].min():.3f}")
    print(f"   MÃ¡xima: {df['confidence'].max():.3f}")
    
    # DistribuciÃ³n de scores
    print("\nðŸ“ˆ DISTRIBUCIÃ“N DE TREND SCORES:")
    score_bins = pd.cut(df['trend_score'], bins=[-11, -6, -2, 1, 5, 11], 
                        labels=['Strong Bearish', 'Weak Bearish', 'Neutral', 'Weak Bullish', 'Strong Bullish'])
    score_dist = score_bins.value_counts().sort_index()
    for label, count in score_dist.items():
        pct = (count / len(df)) * 100
        print(f"   {label}: {count} ({pct:.1f}%)")
    
    # AlineaciÃ³n de EMAs
    print("\nðŸ”€ ALINEACIÃ“N DE EMAs:")
    alignment_counts = df['alignment'].value_counts()
    for alignment, count in alignment_counts.items():
        pct = (count / len(df)) * 100
        print(f"   {alignment}: {count} ({pct:.1f}%)")
    
    # Win rate por alineaciÃ³n
    print("\nâœ… WIN RATE POR ALINEACIÃ“N:")
    for alignment in df['alignment'].unique():
        alignment_df = df[df['alignment'] == alignment]
        win_rate = alignment_df['success'].mean() * 100
        print(f"   {alignment}: {win_rate:.1f}%")
    
    # Exchanges/sÃ­mbolos
    print("\nðŸŒ FUENTES DE DATOS:")
    sources = df['source'].value_counts()
    for source, count in sources.items():
        print(f"   {source}: {count} registros")
    
    symbols = df['symbol'].value_counts()
    for symbol, count in symbols.items():
        print(f"   {symbol}: {count} registros")
    
    print("\n" + "=" * 80)


def calculate_pnl(row: Dict) -> float:
    """
    Calcula el PnL en pips desde los datos crudos.
    
    Args:
        row: Registro del dataset
        
    Returns:
        PnL en pips
    """
    pattern = row['pattern_candle']['pattern']
    pattern_close = row['pattern_candle']['close']
    outcome_close = row['outcome_candle']['close']
    
    # Patrones bajistas (SHORT)
    if pattern in ['SHOOTING_STAR', 'HANGING_MAN']:
        return (pattern_close - outcome_close) * 10000
    # Patrones alcistas (LONG)
    else:
        return (outcome_close - pattern_close) * 10000


def advanced_analysis(df: pd.DataFrame):
    """
    AnÃ¡lisis avanzado con cÃ¡lculos derivados.
    
    Args:
        df: DataFrame con el dataset
    """
    print("\n" + "=" * 80)
    print("ðŸ”¬ ANÃLISIS AVANZADO")
    print("=" * 80)
    
    # Extraer campos
    df['pattern'] = df['pattern_candle'].apply(lambda x: x['pattern'])
    df['success'] = df['outcome'].apply(lambda x: x['success'])
    
    # Calcular PnL para todos los registros
    df['pnl_pips'] = df.apply(calculate_pnl, axis=1)
    
    # PnL por patrÃ³n
    print("\nðŸ’° PnL PROMEDIO POR PATRÃ“N:")
    for pattern in df['pattern'].unique():
        pattern_df = df[df['pattern'] == pattern]
        avg_pnl = pattern_df['pnl_pips'].mean()
        total_pnl = pattern_df['pnl_pips'].sum()
        print(f"   {pattern}: {avg_pnl:+.2f} pips promedio | Total: {total_pnl:+.2f} pips")
    
    # PnL acumulado
    df_sorted = df.sort_values('timestamp')
    df_sorted['cumulative_pnl'] = df_sorted['pnl_pips'].cumsum()
    
    print(f"\nðŸ“Š PnL ACUMULADO:")
    print(f"   Inicial: 0.00 pips")
    print(f"   Final: {df_sorted['cumulative_pnl'].iloc[-1]:+.2f} pips")
    print(f"   MÃ¡ximo drawdown: {df_sorted['cumulative_pnl'].min():+.2f} pips")
    print(f"   MÃ¡ximo peak: {df_sorted['cumulative_pnl'].max():+.2f} pips")
    
    # Mejor/peor operaciÃ³n
    best_trade = df.loc[df['pnl_pips'].idxmax()]
    worst_trade = df.loc[df['pnl_pips'].idxmin()]
    
    print(f"\nðŸ† MEJOR OPERACIÃ“N:")
    print(f"   PatrÃ³n: {best_trade['pattern_candle']['pattern']}")
    print(f"   PnL: {best_trade['pnl_pips']:+.2f} pips")
    print(f"   Timestamp: {best_trade['timestamp']}")
    
    print(f"\nðŸ’” PEOR OPERACIÃ“N:")
    print(f"   PatrÃ³n: {worst_trade['pattern_candle']['pattern']}")
    print(f"   PnL: {worst_trade['pnl_pips']:+.2f} pips")
    print(f"   Timestamp: {worst_trade['timestamp']}")
    
    print("\n" + "=" * 80)


def main():
    """FunciÃ³n principal."""
    dataset_path = Path("data/trading_signals_dataset.jsonl")
    
    if not dataset_path.exists():
        print(f"âŒ Dataset no encontrado: {dataset_path}")
        return
    
    print(f"ðŸ“‚ Cargando dataset: {dataset_path}")
    print()
    
    # MÃ©todo 1: Lectura simple
    print("ðŸ”§ MÃ©todo 1: Lectura simple")
    records = read_jsonl_simple(str(dataset_path))
    print(f"âœ… Cargados {len(records)} registros")
    
    if len(records) > 0:
        print("\nðŸ“‹ Estructura del primer registro:")
        print(f"   Claves principales: {list(records[0].keys())}")
        print(f"   Source: {records[0]['source']}")
        print(f"   Symbol: {records[0]['symbol']}")
        print(f"   Pattern: {records[0]['pattern_candle']['pattern']}")
        print(f"   Confidence: {records[0]['pattern_candle']['confidence']:.3f}")
        print(f"   Success: {records[0]['outcome']['success']}")
    
    # MÃ©todo 2: Con pandas (para anÃ¡lisis)
    print("\nðŸ”§ MÃ©todo 2: Lectura con pandas")
    df = read_jsonl_pandas(str(dataset_path))
    print(f"âœ… DataFrame creado: {df.shape[0]} filas Ã— {df.shape[1]} columnas")
    
    # AnÃ¡lisis completo
    if not df.empty:
        analyze_dataset(df)
        advanced_analysis(df)


if __name__ == "__main__":
    main()
